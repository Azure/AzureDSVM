---
title = "Using Azure Data Science Resources: Compute on Linux DSVM Quick Start"
author= "Graham Williams"
---

# Use Case

A common use case is for a Data Scientist to create their R programs
to analyse a dataset on their local compute platform (e.g., a laptop
with 6GB RAM running Ubuntu with R installed). Development is
performed with a subset of the full dataset (a random sample) that
will not exceed the available memory and will return results
quickly. When the experimental setup is complete the script can be
sent across to an considerably more capable compute engine on Azure.

A Linux Data Science Virtual Machine (DSVM) is deployed, the analysis
completed, results collected, and the compute resources deleted. Azure
consumption costs are minimised.

This demonstrate presents how an experimental data analytics can be thrown
onto a Linux DSVM or a customized Linux DSVM set and executed in a desired
high-performance computing context.

# Setup

We assume that the first step of [ConnectToLinuxDSVM](https://github.com/Azure/AzureDSR/vignettes/ConnectToLinuxDSVM.Rmd) has been done, and there is at least one Linux DSVM existing at the created resouce group.

To begin with, let's check the status of the DSVM and start it if it is deallocated. This is achieved with AzureSMR, and again confidentials for authenticating the app in Active Directory should be provided.

```{r setup}
# Load the required subscription resources: TID, CID, and KEY.
# Also includes the ssh PUBKEY for the user.

USER <- Sys.getenv("USER")

source(paste0(USER, "_credentials.R"))
```

```{r credentials, eval=FALSE}
# Credentials come from app creation in Active Directory within Azure.
 
TID <- "72f9....db47"          # Tenant ID
CID <- "9c52....074a"          # Client ID
KEY <- "9Efb....4nwV....ASa8=" # User key
```

```{r packages}
# Load the required packages.

library(AzureSMR)    # Support for managing Azure resources.
library(AzureDSR)    # Further support for the Data Scientist.
library(magrittr)    
library(dplyr)
library(rattle)      # Use weatherAUS as a "large" dataset.
```

```{r tuning}
# Parameters for this script: the name for the new resource group and
# its location across the Azure cloud. The resource name is used to
# name the resource group that we will create transiently for the
# purposes of this script.

RG     <- "my_dsvm_rg_sea"  # Create if not already exist then kill.
LOC    <- "southeastasia"   # Where the resource group (resources) will be hosted.
# LDSVM  <- "mydsvm001"
LDSVM  <- c("zzz001", "zzz002")
VM_URL <- paste(LDSVM, LOC, "cloudapp.azure.com", sep=".")
```

# DSVM Operation

We can operate the created DSVM instance as desired through R.

```{r dsvm operation}
# Authentication.

context <- createAzureContext(tenantID=TID, clientID=CID, authKey=KEY)

# get VM list under the resource group.

vm_names <- 
  AzureSMR::azureListVM(context, RG, LOC) %T>%
  print()

# start the DSVM if it is not running.

operateDSVM(context, RG, LDSVM, operation="Start")

# check status of a DSVM.

operateDSVM(context, RG, LDSVM, operation="Check")

```

# Run analytics.

Next step is to use the DSVM for data analytics.

There are many ways of interacting with a DSVM. For both Linux and Windows based DSVMs, it is convenient to remote login onto the machines with GUI (more detailed information can be found [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-provision-vm)). A lot of times remote execution within R session is preferred by data scientist as it can be efficiently automated by R scripts. The following chunks of codes demonstrate how to use an R interface for remote execution of R scripts under a desired computing context.  

A very simple experiment on random number generation.
```{r set R interface}

# Create a script for remote execution.

codes <- "x <- seq(1, 500); y <- x * rnorm(length(x), 0, 0.1); sprintf('The results is %d', y)"

writeLines(codes, "./experiment1.R")
                 
# local parallelism on node cores.

t1 <- Sys.time()

executeScript(context,
              resourceGroup=RG,
              machines=LDSVM,
              remote=VM_URL[1],
              user="zzzuser",
              script="./experiment1.R",
              master=VM_URL[1],
              slaves=VM_URL[1],
              computeContext="localParallel")

t2 <- Sys.time()

# cluster parallelism across nodes.

executeScript(context,
              resourceGroup=RG,
              machines=LDSVM,
              remote=VM_URL[1],
              user="zzzuser",
              script="./experiment1.R",
              master=VM_URL[1],
              slaves=VM_URL[-1],
              computeContext="clusterParallel")

t3 <- Sys.time()

performance1 <- t2 - t1
performance2 <- t3 - t2

performance1
performance2

```

Yet another example with parallel execution by using `rxExec` function

```{r}

# parallelizing k-means clustering on iris data.

codes <- paste("library(scales)",
               "df <- scale(iris[, -5])",
               "rxExec(kmeans, x=df, centers=2)",
               sep=";")

writeLines(codes, "./experiment2.R")

executeScript(context,
              resourceGroup=RG,
              machines=LDSVM,
              remote=VM_URL[1],
              user="zzzuser",
              script="./experiment2.R",
              master=VM_URL[1],
              slaves=VM_URL[-1],
              computeContext="clusterParallel")

```

Clean up.

```{r}

file.remove("./experiment1.R", "./experiment2.R")

```
