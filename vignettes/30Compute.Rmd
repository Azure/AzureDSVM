---
title = "Using Azure Data Science Virtual Machine: Compute on Linux DSVM Quick Start"
author= "Graham Williams"
output: rmarkdown::html_vignette
vignette: >
 %\VignetteIndexEntry{Vignette Title}
 %\VignetteEngine{knitr::rmarkdown}
 \usepackage[utf8]{inputenc}
---

# Use Case

A common use case is for a Data Scientist to create their R programs
to analyse a dataset on their local compute platform (e.g., a laptop
with 6GB RAM running Ubuntu with R installed). Development is
performed with a subset of the full dataset (a random sample) that
will not exceed the available memory and will return results
quickly. When the experimental setup is complete the script can be
sent across to a considerably more capable compute engine on Azure.

In this vignette a Linux Data Science Virtual Machine (DSVM) cluster
is deployed, a distributed/parallel analysis is completed, results
collected, and the compute resources deleted. Azure consumption occurs
just for the duration. 

# Setup

```{r setup}
# Load the required subscription resources: TID, CID, and KEY.
# Also includes the ssh PUBKEY for the user.

USER <- Sys.info()[['user']]

source(paste0(USER, "_credentials.R"))
```

```{r packages}
# Load the required packages.

library(AzureSMR)    # Support for managing Azure resources.
library(AzureDSVM)    # Further support for the Data Scientist.
library(magrittr)    
library(dplyr)
```

```{r tuning}
# Parameters for this script: the name for the new resource group and
# its location across the Azure cloud. The resource name is used to
# name the resource group that we will create transiently for the
# purposes of this script.

# Create a random resource group to reduce likelihood of conflict with
# other users.

BASE <- 
  runif(4, 1, 26) %>%
  round() %>%
  letters[.] %>%
  paste(collapse="") %T>%
  {sprintf("Base name:\t\t%s", .) %>% cat("\n")}

RG <-
  paste0("my_dsvm_", BASE,"_rg_sea") %T>%
  {sprintf("Resource group:\t\t%s", .) %>% cat("\n")}

# Choose a data centre location.

LOC <-
  "southeastasia"  %T>%
  {sprintf("Data centre location:\t%s", .) %>% cat("\n")}

# Include the random BASE in the hostname to reducely likelihood of
# conflict.

HOST <-
  paste0("my", BASE) %T>%
  {sprintf("Hostname:\t\t%s", .) %>% cat("\n")}

cat("\n")
```

To begin with, let's check the status of the DSVM and start it if it
is deallocated. This is achieved with AzureSMR, and again
confidentials for authenticating the app in Active Directory should be
provided.

```{r connect}
# Connect to the Azure subscription and use this as the context for
# all of our activities.

context <- createAzureContext(tenantID=TID, clientID=CID, authKey=KEY)

# Check if the resource group already exists. Take note this script
# will not remove the resource group if it pre-existed.

rg_pre_exists <- existsRG(context, RG, LOC) %T>% print()

if (! rg_pre_exists)
{
  # Create a new resource group into which we create the VMs and
  # related resources. Resource group name is RG. 
  
  # Note that to create a new resource group one needs to add access
  # control of Active Directory application at subscription level.

  azureCreateResourceGroup(context, RG, LOC)

}
```

# Deploy the VM Cluster

```{r deploy a cluster of DSVMs}
# Deploy a cluster of 3 DSVMs.

COUNT <- 3

cluster <- deployDSVMCluster(context, 
                             resource.group=RG, 
                             location=LOC, 
                             hostnames=BASE,
                             usernames=USER, 
                             pubkeys=PUBKEY,
                             count=COUNT)

for (i in 1:COUNT)
{
  vm   <- cluster[i, "hostname"]
  fqdn <- cluster[i, "fqdn"]
  
  cat(vm, "\n")
  
  operateDSVM(context, RG, vm, operation="Check")

  cmd <- paste("ssh -q",
               "-o StrictHostKeyChecking=no",
               "-o UserKnownHostsFile=/dev/null",
               fqdn, "uptime")
  cmd
  system(cmd, intern=TRUE) %>% print()
}
```

# Run analytics.

Next step is to use the DSVM for data analytics.

There are many ways of interacting with a DSVM. For both Linux and
Windows based DSVMs, it is convenient to remote login onto the
machines with GUI (more detailed information can be found
[here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-provision-vm)). A
lot of times remote execution within R session is preferred by data
scientist as it can be efficiently automated by R scripts. The
following chunks of codes demonstrate how to use an R interface for
remote execution of R scripts under a desired computing context.

A very simple experiment on random number generation. The function `executeScript` handles the remote execution (Note the current version only supports remote execution of script on a Linux DSVM). Computing context can be specified for the execution. In the case of "clusterParallel", a cluster of DSVMs are used.

```{r set R interface}

# Create a script for remote execution.

code <- "
x <- seq(1, 500)
y <- x * rnorm(length(x), 0, 0.1)
print(y)
"

tmpf1 <- tempfile(paste0("AzureDSVM_experiment_01_"))
file.create(tmpf1)
writeLines(code, tmpf1)
                 
# local parallelism on node cores.

t1 <- Sys.time()

executeScript(context,
              resourceGroup=RG,
              machines=cluster$hostname[1],
              remote=cluster$fqdn[1],
              user=unique(cluster$username),
              script=tmpf1,
              master=cluster$fqdn[1],
              slaves=cluster$fqdn[1],
              computeContext="localParallel")

t2 <- Sys.time()

# cluster parallelism across nodes.

executeScript(context,
              resourceGroup=RG,
              machines=cluster$hostname,
              remote=cluster$fqdn[1],
              user=unique(cluster$username),
              script=tmpf1,
              master=cluster$fqdn[1],
              slaves=cluster$fqdn[-1],
              computeContext="clusterParallel")

t3 <- Sys.time()

performance1 <- t2 - t1
performance2 <- t3 - t2

performance1
performance2

```

Yet another example with parallel execution by using `rxExec` function from Microsoft RevoScaleR package. 

```{r}

# parallelizing k-means clustering on iris data.

codes <- paste("library(scales)",
               "df <- scale(iris[, -5])",
               "rxExec(kmeans, x=df, centers=2)",
               sep=";")


tmpf2 <- tempfile(paste0("AzureDSVM_experiment_02_"))
file.create(tmpf2)
writeLines(codes, tmpf2)

t4 <- Sys.time()

executeScript(context,
              resourceGroup=RG,
              machines=cluster$hostname,
              remote=cluster$fqdn[1],
              user=unique(cluster$username),
              script=tmpf2,
              master=cluster$fqdn[1],
              slaves=cluster$fqdn[-1],
              computeContext="clusterParallel")

t5 <- Sys.time()

performance3 <- t5 - t4

performance3
```

Clean up.

```{r}

file.remove(tmpf1, tmpf2)

# Delete the resource group now that we have proved existence. There
# is probably no need to wait. Only delete if it did not pre-exist
# this script. Deletion seems to take 10 minutes or more.

if (! rg_pre_exists)
  azureDeleteResourceGroup(context, RG)

```
