---
title = "Use case study" 
author= "Graham Williams"
---

This is a demonstration of using `AzureDSR` for running experimentation with DSVM(s) deployed on demand, and getting the cost summarization for running such analytic jobs. 

# Introduction

```{r}
library(AzureDSR)
library(AzureSMR)
library(dplyr)
library(stringr)
library(stringi)
library(magrittr)
library(readr)
library(rattle)
```

# Azure resource invokation

Let's repeat the same thing in the previous tutorials, to deploy DSVMs for the case studies. For comparison, two scenarios, single DSVM and cluster of DSVMs, are deployed under two resource groups. 

```{r credentials, eval=FALSE}
# Credentials come from app creation in Active Directory within Azure.
 
TID <- "72f9....db47"          # Tenant ID
CID <- "9c52....074a"          # Client ID
KEY <- "9Efb....4nwV....ASa8=" # User key

PUBKEY   <- readLines("~/.ssh/id_rsa.pub") # For Linux DSVM
PASSWORD <- "AmSj&%4aR3@kn"                # For Windows DSVM

```

Save such information into a file with the name <USER>_credentials.R
where <USER> is replaced with your username. Then we simply source
that file in R.

```{r setup}
# Load the required subscription resources: TID, CID, and KEY.
# Also includes the ssh PUBKEY for the user.

USER <- Sys.getenv("USER")

source(paste0(USER, "_credentials.R"))
```

```{r tuning}

RG    <- "my_dsvm_rg_sea"  # Will be created if not already exist then kill.
LOC   <- "southeastasia"   # Where the resource group (resources) will be hosted.
LDSVM <- "myldsvm"
LDSVMS <- paste0(LDSVM, as.character(seq(1, 5, 1))) # let's deploy a 5 node cluster.
```

Check existence of resource group and create one if there is no.

```{r connect}

context <- createAzureContext(tenantID=TID, clientID=CID, authKey=KEY)

rg_pre_exists <- existsRG(context, RG, LOC)
```

## Create a Resource Group

Create the resource group within which all resources we create will be
grouped.

```{r create resource group}
if (! rg_pre_exists) azureCreateResourceGroup(context, RG, LOC)

existsRG(context, RG, LOC)
```

## Deploy a Linux Data Science Virtual Machine

Create the actual Linux DSVM with public-key based authentication
method. Name, username, and size can also be configured.

```{r deploy}

# Create the required Linux DSVM - generally 4 minutes.

ldsvm <- deployDSVM(context, 
                    resource.group=RG,
                    location=LOC,
                    name=LDSVM,
                    username=USER,
                    size="Standard_DS1_v2",
                    os="Linux",
                    authen="Key",
                    pubkey=PUBKEY)
ldsvm

operateDSVM(context, RG, LDSVM, operation="Check")

azureListVM(context, RG)
```

## Deploy a cluster of Linux Data Science Virtual Machines.

```{r}

# Create a set of Linux DSVMs and they will be formed as a cluster.

ldsvm_set <- deployDSVMCluster(context, 
                               resource.group=RG, 
                               location=LOC, 
                               count=COUNT, 
                               name=LDSVM, 
                               username=LUSER, 
                               pubkey=rep(PUBKEY, COUNT), 
                               cluster=FALSE)
```

# Analytics

For a pair comparison, several data sets with various sizes and characteristics are used. No prior knowledge (from domain) is assumed for manipulating these data, so merely generic data science techniques are applied on data sets for creating a model for a specific task.

Without loss of generality, each of the analytics is formalized to follow a typical data science work flow:

1. **Feature Engineering**. Note as there is no prior domain knowledge, data used for analytics are assumed to have been aggregated and cleansened properly. General feature engineering techniques are applied on the data. To name a few, 
2. **Algorithm Selection**. Selection of an appropriate algorithm to create model for data science task.
3. **Parameter Tuning**. Tune machine learning algorithms to meet the optimization goal of trained model.

```{r}
url_hr <- "https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-HR-Employee-Attrition.xlsx"

download.file(url=url_hr, 
              destfile="./data.xlsx", 
              mode="wb")

df <- read.xlsx("./data.xlsx", 
                sheetIndex=1,
                header=TRUE,
                colClasses=NA)

head(df)

glimpse(df)
```

While in a real-world use case scenario, there might be necessity for iterating on all of the three steps (sometimes the problem is even more complicated as the work flow is not unidirectional). For simplicity reason, the demonstration solely shows an experimental analytical job on model selection, while those variables related to other two parts are kept fixed.

The following is a sample worker script that will be thrown onto the deployed virtual machines for remote execution. Algorithms used for model creation are `rxDForest` and `rxBTrees` from RevoScaleR and `rxFastTrees` from MicrosoftML.

### Feature engineering

```{r, feature engineering}

# do something such as replacing NA, removing constants, normalization, etc.

# codes here...

```

### Algorithm selection

Different machine learning algorithms are compared in this part. As aforementioned, the algorithm parameters are kept constant but they may be variables as well depending on requirements.

```{r, algorithm selection and parameter tuning}

# make a model config to temporarily preserve model parameters. Parameters are kept fixed.

model_config <-
  list(
    rxDForest = list(seed=10,
                     cp=0.01,
                     nTree=100,
                     mTry=3),
    rxBTrees = list(seed=10,
                    learningRate=0.2,
                    nTree=100,
                    minSplit=10,
                    minBucket=10),
    rxFastTrees = list(numTrees=100,
                       numLeaves=10,
                       learningRate=0.2)
  )

# functions used for model building and evaluating.

modelCreation <- function(formula, dataTrain, dataTest, modelName, modelConfig) {
  
  # function to evaluate the model performance.
  
  modelEvaluation <- function(observed, predicted) {
    confusion <- table(observed, predicted)
    print(confusion)
    tp <- confusion[1, 1]
    fn <- confusion[1, 2]
    fp <- confusion[2, 1]
    tn <- confusion[2, 2]
    accuracy <- (tp + tn) / (tp + fn + fp + tn)
    precision <- tp / (tp + fp)
    recall <- tp / (tp + fn)
    fscore <- 2 * (precision * recall) / (precision + recall)
    metrics <- c("Accuracy"=accuracy,
                 "Precision"=precision,
                 "Recall"=recall,
                 "F-Score"=fscore)
    print(data.frame(metrics))
    return(metrics)
  }
  
  model <- do.call(modelName, c(list(formula=formula, 
                                     data=dataTrain,
                                     verbose=2),
                                modelConfig))
  
  label <- all.vars(formula)[1]
  
  prediction <- 
    rxPredict(modelObject=model,
              data=dataTest,
              type="prob",
              overwrite=TRUE) 
  prediction <-
    ifelse(prediction[,1] > 0.5, "Yes", "No") %>%
    lapply(. , factor, c(levels=c("No", "Yes"))) %>%
    unlist()
  
  pred_metrics <- modelEvaluation(observed=unlist(select(dataTest, contains(label))),
                                  predicted=prediction)
  
  return(list(model=modelName, 
              parameters=modelConfig, 
              results=pred_metrics))
}
```

Following are codes that run analytics defined above as a declarative machine learing process. The whole analytic will be run in a distributed manner across the deployed Azure resources, with a computing context defined in the compute interface. For the convenience of comparative study, model evaluation is embedded into the analytic as well so that the final output will yield performance using different algorithms measured in specified metrics such as accuracy and precision.

```{r}
# ciData is a reference to data source for the analytic job. It is automatically generated by updateScript function in AzureDSR with the help of a configured compute interface.

df <- ciData

train_index <- sample(1:nrow(df), round(0.7 * nrow(df)))
df_train <- df[train_index, ]
df_test <- df[-train_index, ]

# execute the analytics with specified computing context for boosted performance.

names_train <- rxGetVarNames(data=select(df_train, -Attrition))
formula <- as.formula(paste("Attrition ~ ", paste(names_train, collapse="+")))

results <- 
  rxExec(modelCreation,
         formula=formula,
         dataTrain=df_train,
         dataTest=df_test,
         modelName=rxElemArg(names(model_config)),
         modelConfig=rxElemArg(model_config)) %T>%
  print()

```

The worker script can be simply executed on a remote Linux DSVM or DSVM cluster with AzureDSR function `executeScript` like what has been done in the previous tutorials. Again, for comparison purpose, the script is executed on a single DSVM and a cluster of DSVMs.

Time point of running the analytics are captured for reference later on.

```{r execution}

# remote execution on a single DSVM.

time1 <- Sys.time()

executeScript(context,
              resourceGroup=RG,
              machines=LDSVM,
              remote=VM_URL,
              user=USER,
              script="./workerScript1.R",
              master=VM_URL,
              slaves=VM_URL,
              computeContext="localParallel")

# remote execution on a cluster of DSVMs.

time2 <- Sys.time()

executeScript(context,
              resourceGroup=RG,
              machines=LDSVMS,
              remote=VMS_URL[1],
              user=USER,
              script="./workerScript2.R",
              master=VMS_URL[1],
              slaves=VMS_URL[-1],
              computeContext="clusterParallel")

time3 <- Sys.time()

```

# Calculating expense

After execution of the analytic job is done, expense on running the executions on Azure resources cannot be obtained.

## Expense of using a single DSVM. 

Firstly basic information for calculing expense should be provided.

```{r}
VM     <- LDSVM
START  <- time1
END    <- time2
GRA    <- "Hourly"

CURR   <- "USD"
LOCALE <- "en-SG"
REG    <- "SG"
OFFER  <- "MS-AZR-0015P"
```

After that just simply call the `expenseCalculator` function to get the expense of the instance. 

```{r}
consum <- 
  expenseCalculator(context,
                            instance=VM,
                            timeStart=START,
                            timeEnd=END,
                            granularity=GRA,
                            currency=CURR,
                            locale=LOCALE,
                            offerId=OFFER,
                            region=REG) %T>%
  print()
```

# Clean-up
