---
title = "Use case study - clustering analysis" 
author= "Le Zhang and Graham Williams"
---

This is a demonstration of using `AzureDSR` for running
experimentation with DSVM(s) deployed on demand, and getting the cost
summarization for running such analytic jobs.

# Introduction

```{r}
library(AzureDSR)
library(AzureSMR)
library(dplyr)
library(stringr)
library(stringi)
library(magrittr)
library(readr)
library(rattle)
library(ggplot2)
```

# Azure resource invokation

Initial set up for resource management. The required information can
be provided in a separate file for each user, named with their user
name and the "_credentials.R". That file would contain something like
the following lines of R code.

```{r credentials, eval=FALSE}
TID <- "72f9....db47"          # Tenant ID
CID <- "9c52....074a"          # Client ID
KEY <- "9Efb....4nwV....ASa8=" # User key

PUBKEY   <- readLines("~/.ssh/id_rsa.pub") # For ssh public key.
```

```{r setup}
# Load the required subscription resources: TID, CID, and KEY.
# Also includes the ssh PUBKEY for the user.

USER <- Sys.info()[['user']]

source(paste0(USER, "_credentials.R"))
```

We specify the number of VMs to deploy, choosing just a small number
for demonstration purposes.

Also identify the VM size. Notice our default choice of the size of
the VMs is chosen as a compromise between price and performance. Users
can choose based on their own requirements.

A random name is also generated for the VMs and the resource group that
will house the VMs.

```{r}
COUNT <- 4  # Number of VMs to deploy.

SIZE <- "Standard_DS2_v2"

BASE  <- 
  runif(4, 1, 26) %>%
  round() %>%
  letters[.] %>%
  paste(collapse="") %T>%
  {sprintf("Base name:\t\t%s", .) %>% cat("\n")}

RG <-
  paste0("my_dsvm_", BASE,"_rg_sea") %T>%
  {sprintf("Resource group:\t\t%s", .) %>% cat("\n")}

# Choose a data centre location.

LOC <-
  "southeastasia"  %T>%
  {sprintf("Data centre location:\t%s", .) %>% cat("\n")}

# Include the random BASE in the hostname to reducely likelihood of
# conflict.

HOST <-
  paste0("my", BASE) %T>%
  {sprintf("Hostname:\t\t%s", .) %>% cat("\n")}

cat("\n")
```

Deploy a cluster of DSVMs if there is no existing, otherwise start the
machines. 

```{r}

# --------------------------------------------------------------------------
# Azure data science resource management
# --------------------------------------------------------------------------

# Connect to the Azure subscription and use this as the context for
# all of our activities.

context <- createAzureContext(tenantID=TID, clientID=CID, authKey=KEY)

# Check if the resource group already exists. Take note this script
# will not remove the resource group if it pre-existed.

rg_pre_exists <- existsRG(context, RG, LOC) %T>% print()

# Create Resource Group

if (! rg_pre_exists)
{
  # Create a new resource group into which we create the VMs and
  # related resources. Resource group name is RG. 
  
  # Note that to create a new resource group one needs to add access
  # control of Active Directory application at subscription level.
  
  azureCreateResourceGroup(context, RG, LOC)
  
}

vm <- AzureSMR::azureListVM(context, RG)

if (!is.null(vm))
{
  
  AzureDSR::operateDSVM(context, RG, vm$name, operation="Check")
  
  # start machines if they exist in the resource group.
  
  AzureDSR::operateDSVM(context, RG, vm$name, operation="Start")
  
} else
{
  
  # Create a cluster of Linux Data Science Virtual Machines.
  
  cluster <- deployDSVMCluster(context, 
                               resource.group=RG, 
                               size=SIZE,
                               location=LOC, 
                               hostnames=BASE,
                               usernames=USER, 
                               pubkeys=PUBKEY,
                               count=COUNT)

  # Confirm that each VM exists.
  
  for (i in 1:COUNT)
  {
    vm <- cluster[i, "hostname"]
    fqdn <- cluster[i, "fqdn"]
    
    cat(vm, "\n")
    
    operateDSVM(context, RG, vm, operation="Check")
    
    # Send a simple system() command across to the new server to test
    # its existence. Expect a single line with an indication of how long
    # the server has been up and running.
    
    cmd <- paste("ssh -q",
                 "-o StrictHostKeyChecking=no",
                 "-o UserKnownHostsFile=/dev/null\\\n   ",
                 fqdn,
                 "uptime") %T>%
                 {cat(., "\n")}
    cmd
    system(cmd)
    cat("\n")
  }
}
```

The demo analytics demonstrates a parallel cluster analysis using the
kmeans algorithm for clustering unlabelled data. The data consists of
credit transactions as available from
[kaggle website](https://www.kaggle.com/dalpozz/creditcardfraud) or
directly from
[togaware]{https://access.togaware.com/creditcard.csv}. The original
data are labelled and so in our clustering analysis the label is
removed.

The R code for clustering is available from github as
[worker_cluster.R]{...test/worker_cluster.R}. The analysis basically
normalises the credit transaction data and then performs 10 repeated
clustering analyses (targeting 2 clusters) for each using the k-means
algorithm. The repetition is completed in parallel with the specified
computing context which is available in `RevoScaleR` package. Note the
computing context information will be automatically added by the
`executeScript` function given a specified computing context.

TODO DOWNLOAD SCRIPT TO EACH VM

The script can then be saved and later on path to the script is used
as reference. For example, in this demo, the script is saved with name
"worker_cluster.R" which is located in the "/test" directory.

The following code is to run the clustering analysis on a specified
computing environment. This is achieved by setting computing
context. For comparison purpose, two contexts, "localParallel" and
"clusterParallel" are used in the demo.

```{r}
# specify machine names, master, and slaves.

machines <- unlist(vm$name)
dns_list <- paste0(machines, ".", LOC, ".cloudapp.azure.com")
master <- dns_list[1]
slaves <- dns_list[-1]
```

```{r}
# parallel the analytics with local parallel computing context.

time_1 <- Sys.time()

AzureDSR::executeScript(context=context, 
                        resourceGroup=RG, 
                        machines=machines, 
                        remote=master, 
                        user=USER, 
                        script="./test/worker_cluster.R", 
                        master=master, 
                        slaves=slaves, 
                        computeContext="localParallel")

time_2 <- Sys.time()

# get results from remote

AzureDSR::fileTransfer(from=paste0(master, ":~"), 
                       to=".", 
                       user=USER, 
                       file="results.RData")

load("./results.RData") 
results_local <- 
  results %T>%
  print()
```

```{r}
# parallel the analytics across cluster.

time_3 <- Sys.time()

AzureDSR::executeScript(context=context, 
                        resourceGroup=RG, 
                        machines=machines, 
                        remote=master, 
                        user=USER, 
                        script="./test/worker_cluster.R", 
                        master=master, 
                        slaves=slaves, 
                        computeContext="clusterParallel")

time_4 <- Sys.time()

# get results from remote

AzureDSR::fileTransfer(from=paste0(master, ":~"), 
                       to=".", 
                       user=USER, 
                       file="results.RData")

load("./results.RData") 
results_cluster <- results
```

Do some visualization with data.

```{r}
DATA_URL <- "https://zhledata.blob.core.windows.net/mldata/creditcard.xdf"

credit_data <- rxImport(inData=DATA_URL,
                        missingValueString="M",
                        stringsAsFactors=FALSE,
                        overwrite=TRUE)

# select one clustering result from results_local.

cluster_local <- 
  results_local[[1]] %>%
  factor()

# visualize first two dimensions as jitter plot.

ggplot(data=credit_data, aes(x=V1, y=V2, color=cluster_local)) +
  geom_jitter() +
  stat_ellipse(geom="polygon", alpha=0.5, aes(fill=cluster_local)) 
```

Once finishing the analysis, switch off DSVMs.

```{r}
# stop machines after the analysis.

AzureDSR::operateDSVM(context, RG, vm$name, operation="Stop")
```

The cost of running the above analytics can be obtained with
`expenseCalculation` function, but one thing worthing noting is that
there is usually delay between execution of jobs and record of data
consumption. The delay varies across regions of data centers, so it is
recommended to save starting and ending time points of analytical jobs
for reference so that later on `expenseCalculator` can be safely
called for retrieving results.

```{r}
# calculate expense on computations. 
#

cost <- 0

if (length(vm$name) == 1) {
  cost <- AzureDSR::expenseCalculator(context=context,
                                      instance=as.character(vm$name[1]), 
                                      timeStart=time_1,
                                      timeEnd=time_2,
                                      granularity="Hourly",
                                      currency="USD",
                                      locale="en-SG",
                                      offerId="MS-AZR-0015P",
                                      region="SG")
} else {
  for (name in as.character(vm$name)) {
    cost <- cost + AzureDSR::expenseCalculator(context=context,
                                               instance=name, 
                                               timeStart=time_1,
                                               timeEnd=time_2,
                                               granularity="Hourly",
                                               currency="USD",
                                               locale="en-SG",
                                               offerId="MS-AZR-0015P",
                                               region="SG")
  }
}
```
