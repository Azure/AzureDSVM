---
title = "Using Azure Data Science Virtual Machine: a use case - hotspot" 
author= "Le Zhang and Graham Williams"
output: rmarkdown::html_vignette
vignette: >
 %\VignetteIndexEntry{Vignette Title}
 %\VignetteEngine{knitr::rmarkdown}
 \usepackage[utf8]{inputenc}
---

# Use Case

This tutorial will demonstrate the effectiveness of parallelizing a [Hot Spots](https://togaware.com/papers/pakdd99.pdf) analysis across Azure DSVM(s). 
The Hot Spots method was proposed by Graham Williams for discovering knowledge of interets from large data sets. It consists of three steps:

    1. Cluster a data set into several complete and disjoint clusters. This is often achieved with a clustering algorithm such as k-means.
    2. Rule-based inductions are then built to discriminatorily describe each of the clusters. 
    3. Within each of the clusters specified by associative rules, create models to evaluate data attributes to find patterns.

The greatest benefit of using Hot Spots method for data mining are that it visually describes the knowledge by a set of rules which are of particular convenience to a data miner to understand mining results. This is helpful in various scenarios such as insurance premium setting, fraud detection in health, etc.

In this demonstration, Hotspots analysis is used for supervised binary classification. The workflow is as follows

    0. Given a labelled data set. Split the data into training and testing sets.
    1. For the training set, cluster it into different segments. This is done by k-means algorithm.
        1. Number of clusters is swept from 2 to a large enough number (say 20 in this demo) in order to select the optimal one. The selection is based on [elbow method](https://www.google.com.sg/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjDo7W43InTAhXEllQKHZc1CowQygQIIjAB&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FDetermining_the_number_of_clusters_in_a_data_set%23The_elbow_method&usg=AFQjCNEsAz8DP6f5iihKrOR6qT_hRDUdGw&sig2=XQySMd1rPShm8IRdwlFqRw). 
        2. Considering that k-means clustering performance relies on initial conditions, each clustering is run 10 times for averging out instability. 
    2. Assume the number of clusters is determined to be N. Build N descriptive models that differentiate each cluster from the rest. Within each cluster, build a predictive model to do binary classification on the data label.
    3. In the testing phase, examine the each observation of the data which cluster it belongs to using the descriptive models. Then do binary classification using the predictive models to produce the predicted label.

# Setup

Similar to the previous sections, credentials for authentication are required to fire up the DSVMs.

```{r}
library(AzureDSVM)
library(AzureSMR)
library(dplyr)
library(stringr)
library(stringi)
library(magrittr)
library(readr)
library(rattle)
library(ggplot2)
library(wskm)
```

```{r setup}
# Load the required subscription resources: TID, CID, and KEY.
# Also includes the ssh PUBKEY for the user.

USER <- Sys.info()[['user']]

source(paste0(USER, "_credentials.R"))
```

Specifications of computing resources.

```{r}
runif(4, 1, 26) %>%
  round() %>%
  letters[.] %>%
  paste(collapse="") %T>%
  {sprintf("Base name:\t\t%s", .) %>% cat("\n")} ->
BASE

BASE %>%
  paste0("my_dsvm_", .,"_rg_sea") %T>%
  {sprintf("Resource group:\t\t%s", .) %>% cat("\n")} ->
RG

SIZE <- "Standard_D12_v2" 
sprintf("DSVM size:\t\t%s", SIZE) %>% cat("\n")

# Choose a data centre location.

"southeastasia"  %T>%
  {sprintf("Data centre location:\t%s", .) %>% cat("\n")} ->
LOC

# Include the random BASE in the hostname to reducely likelihood of
# conflict.

BASE %>%
  paste0("my", .) %T>%
  {sprintf("Hostname:\t\t%s", .) %>% cat("\n")} ->
HOST

cat("\n")
```

Deploy a cluster of DSVMs if there is no existing, otherwise start the
machines. 

```{r}
# Connect to the Azure subscription and use this as the context for
# all of our activities.

context <- createAzureContext(tenantID=TID, clientID=CID, authKey=KEY)

# Check if the resource group already exists. Take note this script
# will not remove the resource group if it pre-existed.

rg_pre_exists <- existsRG(context, RG, LOC) %T>% print()

# Create Resource Group

if (! rg_pre_exists)
{
  # Create a new resource group into which we create the VMs and
  # related resources. Resource group name is RG. 
  
  # Note that to create a new resource group one needs to add access
  # control of Active Directory application at subscription level.
  
  azureCreateResourceGroup(context, RG, LOC)
  
}
```

Create one remote DSVM for running the Hotspots analytics.

```{r}
vm <- AzureSMR::azureListVM(context, RG)

if (!is.null(vm))
{
  
  AzureDSVM::operateDSVM(context, RG, vm$name, operation="Check")
  
  # start machines if they exist in the resource group.
  
  AzureDSVM::operateDSVM(context, RG, vm$name, operation="Start")
  
} else
{
  ldsvm <- deployDSVM(context, 
                      resource.group=RG,
                      location=LOC,
                      hostname=HOST,
                      username=USER,
                      size=SIZE,
                      pubkey=PUBKEY)
  ldsvm
  
  operateDSVM(context, RG, HOST, operation="Check")
  
}

azureListVM(context, RG)
```

# Remote execution of analysis

The following demo shows how to achieve a Hot Spots analysis in R and parallelize the analysis on Azure cloud.

The data used in this demonstration is still the credit card data, which is  
available on [kaggle website](https://www.kaggle.com/dalpozz/creditcardfraud) or
directly from
[togaware]{https://access.togaware.com/creditcard.xdf} in XDF format. 

The R codes for Hot Spot analysis are available as [workerHotSpots.R](https://www.microsoft.com). Besides the main worker script, there are several other scripts where functions used for the analysis reside.

    * [workerHotSpotsSetup.R](https://github.com/Azure/AzureDSVM/blob/master/test/workerHotspotsSetup.R) function that sets up the environment.
    * [workerHotSpotsFuncs.R](https://github.com/Azure/AzureDSVM/blob/master/test/workerHotspotsFuncs.R) functions used in training or testing process.
    * [workerHotSpotsTrain.R](https://github.com/Azure/AzureDSVM/blob/master/test/workerHotspotsTrain.R) functions for training a Hot Spot model for binary classification. 
    * [workerHotSpotsTest.R](https://github.com/Azure/AzureDSVM/blob/master/test/workerHotspotsTest.R) functions for testing performance of a classification model based on Hot Spots method.
    * [workerHotSpotsProcess.R](https://github.com/Azure/AzureDSVM/blob/master/test/workerHotspotsProcess.R) a function for the whole process of Hot spots method.
    * [workerHotSpots.R](https://github.com/Azure/AzureDSVM/blob/master/test/workerHotspots.R) top-level script for Hot spots analysis.

The following is the configuration of computing cluster which is needed for specifying a "clusterParallel" computing context. 

    * `machines` names of DSVMs used for parallelisation.
    * `dns_list` DNS of DSVMs. 
    * `master` DNS of the DSVM where the worker script will be uploaded to for execution.
    * `slaves` DNS of DSVMs where execution of worker script will be distributed to.

```{r}
# specify machine names, master, and slaves.

vm <- AzureSMR::azureListVM(context, RG)

machines <- unlist(vm$name)
dns_list <- paste0(machines, ".", LOC, ".cloudapp.azure.com")
master <- dns_list[1]
slaves <- dns_list[-1]
```

The following codes run the analytics of the worker script on a remote DSVM in a "local parallel" computing context, and obtain results from remote master node to local R session.

Since the functions used for the analysis are defined in separated scripts, these scripts are uploaded onto remote DSVM.

```{r}
worker_scripts <- c("workerHotspotsFuncs.R", 
                    "workerHotspotsSetup.R", 
                    "workerHotspotsTrain.R", 
                    "workerHotspotsTest.R",
                    "workerHotspotsProcess.R")

sapply(worker_scripts, 
       fileTransfer,
       from="../test",
       to=paste0(master, ":~"), 
       user=USER)
```

Remote execution of worker script (the analysis takes approximately 10 - 15 minutes on a D12 v2 DSVM which has 4 fores and 28 GB RAM.

```{r}
# parallel the analytics with local parallel computing context.

time_1 <- Sys.time()

AzureDSVM::executeScript(context=context, 
                         resource.group=RG, 
                         machines=machines, 
                         remote=master, 
                         user=USER, 
                         script="../test/workerHotspots.R", 
                         master=master, 
                         slaves=slaves, 
                         compute.context="localParallel")

time_2 <- Sys.time()

# get results from remote

AzureDSVM::fileTransfer(from=paste0(master, ":~"), 
                        to=".", 
                        user=USER, 
                        file="results.RData")

load("./results.RData") 
results_local <- 
  results %T>%
  print()
```

Save time points for later reference.

```{r}
save(list(time_1, time_2), "./elapsed.RData")
```

The cost of running the above analytics can be obtained with
`expenseCalculation` function. 

```{r}
# calculate expense on computations.

load("./elapsed.RData")

cost <- 0

if (length(vm$name) == 1) {
  cost <- AzureDSVM::expenseCalculator(context=context,
                                       instance=as.character(vm$name[1]), 
                                       time.start=time_1,
                                       time.end=time_2,
                                       granularity="Hourly",
                                       currency="currency",
                                       locale="your_locale",
                                       offerId="your_offer_id",
                                       region="your_location")
} else {
  for (name in as.character(vm$name)) {
    cost <- cost + AzureDSVM::expenseCalculator(context=context,
                                               instance=name, 
                                               time.start=time_1,
                                               time.end=time_2,
                                               granularity="Hourly",
                                               currency="currency",
                                               locale="your_locale",
                                               offerId="your_offer_id",
                                               region="your_location")
  }
}
```

# Clean-up

Once finishing the analysis, switch off DSVMs.

```{r}
# stop machines after the analysis.

AzureDSVM::operateDSVM(context, RG, vm$name, operation="Stop")
```

Or delete the resource group to avoid unnecessary cost.

```{r}
if (! rg_pre_exists)
  azureDeleteResourceGroup(context, RG)
```