---
title = "Use case study - binary classification" 
author= "Graham Williams"
---

This is a demonstration of using `AzureDSR` for running experimentation with DSVM(s) deployed on demand, and getting the cost summarization for running such analytic jobs. 

# Introduction

```{r}
library(AzureDSR)
library(AzureSMR)
library(dplyr)
library(stringr)
library(stringi)
library(magrittr)
library(readr)
library(rattle)
```

# Azure resource invokation

Let's repeat the same thing in the previous tutorials, to deploy DSVMs for the case studies. For comparison, two scenarios, single DSVM and cluster of DSVMs, are deployed under two resource groups. 

```{r credentials, eval=FALSE}
# Credentials come from app creation in Active Directory within Azure.
 
TID <- "72f9....db47"          # Tenant ID
CID <- "9c52....074a"          # Client ID
KEY <- "9Efb....4nwV....ASa8=" # User key

PUBKEY   <- readLines("~/.ssh/id_rsa.pub") # For Linux DSVM
PASSWORD <- "AmSj&%4aR3@kn"                # For Windows DSVM

```

Save such information into a file with the name <USER>_credentials.R
where <USER> is replaced with your username. Then we simply source
that file in R.

```{r setup}
# Load the required subscription resources: TID, CID, and KEY.
# Also includes the ssh PUBKEY for the user.

USER <- Sys.getenv("USER")

source(paste0(USER, "_credentials.R"))
```

```{r tuning}

RG    <- "my_dsvm_rg_sea"  # Will be created if not already exist then kill.
LOC   <- "southeastasia"   # Where the resource group (resources) will be hosted.
LDSVM <- "myldsvm"
LDSVMS <- paste0(LDSVM, as.character(seq(1, 5, 1))) # let's deploy a 5 node cluster.
```

Check existence of resource group and create one if there is no.

```{r connect}

context <- createAzureContext(tenantID=TID, clientID=CID, authKey=KEY)

rg_pre_exists <- existsRG(context, RG, LOC)
```

## Create a Resource Group

Create the resource group within which all resources we create will be
grouped.

```{r create resource group}
if (! rg_pre_exists) azureCreateResourceGroup(context, RG, LOC)

existsRG(context, RG, LOC)
```

## Deploy a Linux Data Science Virtual Machine

Create the actual Linux DSVM with public-key based authentication
method. Name, username, and size can also be configured.

```{r deploy}

# Create the required Linux DSVM - generally 4 minutes.

ldsvm <- deployDSVM(context, 
                    resource.group=RG,
                    location=LOC,
                    name=LDSVM,
                    username=USER,
                    size="Standard_DS1_v2",
                    os="Linux",
                    authen="Key",
                    pubkey=PUBKEY)
ldsvm

operateDSVM(context, RG, LDSVM, operation="Check")

azureListVM(context, RG)
```

## Deploy a cluster of Linux Data Science Virtual Machines.

```{r}

# Create a set of Linux DSVMs and they will be formed as a cluster.

ldsvm_set <- deployDSVMCluster(context, 
                               resource.group=RG, 
                               location=LOC, 
                               count=COUNT, 
                               name=LDSVM, 
                               username=LUSER, 
                               pubkey=rep(PUBKEY, COUNT), 
                               cluster=FALSE)
```

# Analytics

## Preliminaries

For a pair comparison, several data sets with various sizes and characteristics are used. No prior knowledge (from domain) is assumed for manipulating these data, so merely generic data science techniques are applied on data sets for creating a model for a specific task.

Without loss of generality, each of the analytics is formalized to follow a typical data science work flow:

1. **Feature Engineering**. Note as there is no prior domain knowledge, data used for analytics are assumed to have been aggregated and cleansened properly. General feature engineering techniques are applied on the data. To name a few, 
2. **Algorithm Selection**. Selection of an appropriate algorithm to create model for data science task.
3. **Parameter Tuning**. Tune machine learning algorithms to meet the optimization goal of trained model.

```{r}
url_hr <- "https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-HR-Employee-Attrition.xlsx"

download.file(url=url_hr, 
              destfile="./data.xlsx", 
              mode="wb")

df <- read.xlsx("./data.xlsx", 
                sheetIndex=1,
                header=TRUE,
                colClasses=NA)

head(df)

glimpse(df)
```

While in a real-world use case scenario, there might be necessity for iterating on all of the three steps (sometimes the problem is even more complicated as the work flow is not unidirectional). For simplicity reason, the demonstration solely shows an experimental analytical job on model selection, while those variables related to other two parts are kept fixed.

The following is a sample worker script that will be thrown onto the deployed virtual machines for remote execution. Algorithms used for model creation are `rxDForest` and `rxBTrees` from RevoScaleR and `rxFastTrees` from MicrosoftML.

### Feature engineering

Feature engineering is always considered to be an art rather than science as it depends on not merely machine learning techniques but also domain knowledge. To a great extent, domain knowledge plays an even more vital role in it. As for illustration purpose, the data sets used in the experiments are pre-processed - there is no need for tasks such as aggregation, normalization, etc. performed on the data sets. The data sets are assumed to be ready for training a binary classsification problem.

### Algorithm selection

Choosing the right algorithm for a machine learning problem is significant to success. There are many models existing off-the-shelf but they are not globally suitable to all sorts of problems. Some may fit to resolve linear problem with desirably good performance but fail in tackling data sets with non-linear correlations across variables. 

To this end, in the step after feature engineering, normally different machine learning algorithms are compared so as to choose the best one. The following is an illustrative model configuration which consists of three different algorithms associated with parameters. The candidature algorithms will be used (with their default paramters) to create multiple models. The models will be validated against testing data, after which certain performance metric such as accuracy, recall, etc., is referenced for selecting the best algorithm.

```{r, algorithm selection and parameter tuning}

# make a model config to temporarily preserve model parameters. Parameters are kept fixed.

model_config <- list(name=c("rxLogit", "rxBTrees", "rxDForest"), 
                     para=list(list(list(maxIterations=10,
                                         coeffTolerance=1e-6),
                                    list(maxIterations=15,
                                         coeffTolerance=2e-6),
                                    list(maxIterations=20,
                                         coeffTolerance=3e-6)),
                               list(list(nTree=10, 
                                         learningRate=0.05),
                                    list(nTree=15,
                                         learningRate=0.1),
                                    list(nTree=20,
                                         learningRate=0.15)),
                               list(list(cp=0.01,
                                         nTree=10,
                                         mTry=3),
                                    list(cp=0.01,
                                         nTree=15,
                                         mTry=3),
                                    list(cp=0.01,
                                         nTree=20,
                                         mTry=3))))
```

### Parameter tuning

After algorithm selection, the next step is to fine tune model hyper parameters so as to finalize the model for the problem. This is the same as the previous step except that the algorithm is fixed and parameters are swept within an acceptable range. The output in this step is the optimal model.

## Sample analysis

The following is a sample script to perform binary classification on a [credit card transaction data set](https://www.kaggle.com/dalpozz/creditcardfraud). The problem is to detect fraud in all transaction records.

Codes of solving such a machine learning problem following the introduced "three-step" procedure are shown as follows. The function `mlProcess` takes data, formula, and model specs as inputs. Considering scalability and performance efficiency, data of xdf format is used, which allows parallel computation outside memory. Area-under-curve is used as performance metric to evaluate quality of model. The function returns a model object (based on the training results) and evaluation result of the model.

```{r}
# functions used for model building and evaluating.

mlProcess <- function(formula, data, modelName, modelPara) {
  
  xdf <- RxXdfData(file=data)
  
  # split data into training set (70%) and testing set (30%).
  
  data_part <- c(train=0.7, test=0.3)
  
  data_split <-
    rxSplit(xdf, 
            outFilesBase=tempfile(),
            splitByFactor="splitVar",
            transforms=list(splitVar=
                              sample(data_factor,
                                     size=.rxNumRows,
                                     replace=TRUE,
                                     prob=data_part)),
            transformObjects=
              list(data_part=data_part,
                   data_factor=factor(names(data_part), levels=names(data_part)))) 
  
  data_train <- data_split[[1]]
  data_test  <- data_split[[2]]
  
  # train model.
  
  if(missing(modelPara) ||
     is.null(modelPara) || 
     length(modelPara) == 0) {
    model <- do.call(modelName, list(data=data_train, formula=formula))
  } else {
    model <- do.call(modelName, c(list(data=data_train,
                                       formula=formula),
                                  modelPara))
  }
  
  # validate model
  
  scores <- rxPredict(model, 
                      data_test,
                      extraVarsToWrite=names(data_test),
                      predVarNames="Pred",
                      outData=tempfile(fileext=".xdf"), 
                      overwrite=TRUE)
  
  label <- as.character(formula[[2]])
  
  roc <- rxRoc(actualVarName=label, 
               predVarNames=c("Pred"), 
               data=scores) 
  
  auc <- rxAuc(roc)
  
  # clean up.
  
  file.remove(c(data_train@file, data_test@file))
  
  return(list(model=model, metric=auc))
}

```

Following are codes that run analytics defined above. The whole analytic will be run in a distributed manner across the deployed Azure resources, with a computing context defined in the compute interface. For the convenience of comparative study, model evaluation is embedded into the analytic as well so that the final output will yield performance using different algorithms measured in specified metrics such as accuracy and precision.

### Step 0 Set up

* Download data into remote machine. 
* Specify computing environment - computing context in Microsoft R Server. This is to enable distribution of analytics across available computing resources.
* Get inputs to `mlProcess` function ready, i.e., formula, data, etc.

```{r}

# -----------------------------------------------------------------------
# Step 0 - let's do some test. Set up the experiment.
# -----------------------------------------------------------------------

# choose a data set in the benchmark by index.

data_index <- 3 
print(data_config[data_index, ])

# read data.

CI_DATA <- data_config$url[data_index]

download.file(CI_DATA,
              destfile="./data.xdf",
              mode="wb")

# download data to all nodes if it is cluster parallel.

if (rxGetComputeContext()@description == "dopar") {
  clusterCall(cl,
              download.file,
              url=CI_DATA,
              destfile="./data.xdf",
              mode="wb")
}

label <- data_config$label[data_index]
label <- as.character(label)

# create a formula.

names <- rxGetVarNames(data="./data.xdf")
names <- names[names != label]
formula <- as.formula(paste(label, "~", paste(names, collapse="+")))
```

### Step 1 - algorithm selection

This part performs algorithm selection on the given candidates.

```{r}
# -----------------------------------------------------------------------
# Step1 - algorithm selection.
# -----------------------------------------------------------------------

# sweep candidate algorithms to select the best one - performance metric such as Area-Under-Curve can be used.

results1 <- rxExec(mlProcess,
                   formula=formula,
                   data="data.xdf",
                   modelName=rxElemArg(model_config$name))

metric1 <- lapply(results1, `[[`, "metric")

algo    <- model_config$name[which(metric1 == max(unlist(metric1)))]
para    <- model_config$para[[which(model_config$name == algo)]]
```

### Step 2 - parameter tuning

After the algorithm is determined in the last step, this step will fine tune hyper parameters of the model.

```{r}
# -----------------------------------------------------------------------
# Step2 - parameter tuning.
# -----------------------------------------------------------------------
# after an algo is selected based on some criterion (let's say AUC, which is a balanced metric that considers both sensitivity and specificity.), another parallel execution on different sets of parameters are run - parameter tuning.

# sweep parameters of the selected algorithm to find the optimal model.

results2 <- rxExec(mlProcess,
                   formula=formula,
                   data="data.xdf",
                   modelName=algo,
                   modelPara=rxElemArg(para))
```

### Step 3 Results collection

The final optimized results will be collected and saved.

```{r}
# select the optimal model with best performance.

metric2    <- lapply(results1, `[[`, "metric")

model_opt  <- results2[[which(metric2 == max(unlist(metric2)))]][["model"]]
metric_opt <- results2[[which(metric2 == max(unlist(metric2)))]][["metric"]]

# save results for reference.

results <- list(model_opt, metric_opt)
save(results, file="./results.RData")

```

As usual, the above analytics can be saved into a single worker script. The worker script can be executed on a remote Linux DSVM or DSVM cluster with AzureDSR function `executeScript` like what has been done in the previous tutorials. 

```{r execution}

# remote execution on a single DSVM.

time1 <- Sys.time()

executeScript(context,
              resourceGroup=RG,
              machines=LDSVM,
              remote=VM_URL,
              user=USER,
              script="./workerScript1.R",
              master=VM_URL,
              slaves=VM_URL,
              computeContext="localParallel")

# remote execution on a cluster of DSVMs.

time2 <- Sys.time()

executeScript(context,
              resourceGroup=RG,
              machines=LDSVMS,
              remote=VMS_URL[1],
              user=USER,
              script="./workerScript2.R",
              master=VMS_URL[1],
              slaves=VMS_URL[-1],
              computeContext="clusterParallel")

time3 <- Sys.time()

```

# Calculating expense

After execution of the analytic job is done, expense on running the executions on Azure resources cannot be obtained.

## Expense of using a single DSVM. 

Firstly basic information for calculing expense should be provided.

```{r}
VM     <- LDSVM
START  <- time1
END    <- time2
GRA    <- "Hourly"

CURR   <- "USD"
LOCALE <- "en-SG"
REG    <- "SG"
OFFER  <- "MS-AZR-0015P"
```

After that just simply call the `expenseCalculator` function to get the expense of the instance. 

```{r}
# calculate expense on computations. 

cost <- 0

if (length(vm$name) == 1) {
  cost <- AzureDSR::expenseCalculator(context=context,
                                      instance=as.character(vm$name[1]), 
                                      timeStart=time_1,
                                      timeEnd=time_2,
                                      granularity="Hourly",
                                      currency="USD",
                                      locale="en-SG",
                                      offerId="MS-AZR-0015P",
                                      region="SG")
} else {
  for (name in as.character(vm$name)) {
    cost <- cost + AzureDSR::expenseCalculator(context=context,
                                               instance=name, 
                                               timeStart=time_1,
                                               timeEnd=time_2,
                                               granularity="Hourly",
                                               currency="USD",
                                               locale="en-SG",
                                               offerId="MS-AZR-0015P",
                                               region="SG")
  }
}
```

# Clean-up
